{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkJxt4yTPOBA"
   },
   "source": [
    "# Scientific document retrieval system for COVID-19 using the  COVID-19 Open Research Dataset (CORD-19)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRSC5XS2fqdI"
   },
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFd89DpZ-VEQ"
   },
   "source": [
    "## About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKhttFg4-Yp0"
   },
   "source": [
    "The [dataset](https://github.com/allenai/cord19) made by Allen AI is a corpus of academic papers about COVID-19 and related coronavirus research. \n",
    "The related paper explains the methodology (Lucy Lu Wang et al., 2020) [link text](https://www.aclweb.org/anthology/2020.nlpcovid19-acl.1/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D0l97Ki7VhZ"
   },
   "source": [
    "## script to download and extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE2vRezI7x0D"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "import os\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEQ207zmOHvD"
   },
   "outputs": [],
   "source": [
    "# dowload and extract\n",
    "def download_CORD19_dataset(version, url=\"https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_{version}.tar.gz\"):\n",
    "    url = url.format(version=version)\n",
    "    ziped_file_name = \"cord-19_\"+version+\".tar.gz\"\n",
    "    file_name = version\n",
    "\n",
    "    # download dataset\n",
    "    if not os.path.exists(ziped_file_name):\n",
    "        print(\"Downloading: \"+url)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url, ziped_file_name)\n",
    "    else:\n",
    "        print(\"Dataset already downloaded\")\n",
    "\n",
    "    # extract dataset\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"Unzipping dataset file\")\n",
    "        import tarfile\n",
    "        tar = tarfile.open(ziped_file_name)\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"Dataset already unziped\")\n",
    "\n",
    "    # extract json files\n",
    "    if not os.path.exists(file_name+\"/document_parses\"):\n",
    "        print(\"Unzipping dataset document parses\")\n",
    "        import tarfile\n",
    "        tar = tarfile.open(file_name+\"/document_parses\"+\".tar.gz\")\n",
    "        tar.extractall(path=\"./\"+file_name)\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"Document parses already unziped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssq64si0cxjR"
   },
   "source": [
    "## Pytorch dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IZIhEsmc1Cp"
   },
   "source": [
    "We will create a dataset class that access the files of the documents we want directly without needing to open all the files, to save memory from this enormous dataset.\n",
    "\n",
    "The method of opening the CSV file and extracting the main body can be found [here](https://github.com/allenai/cord19)\n",
    "\n",
    "The dataset indexes will return the part of the document that the user wants to.\n",
    "\n",
    "For example, if we set the part of doc to be \"title\" then dataset[3] will return the title of the 3rd document as a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha8fMackklMP"
   },
   "source": [
    "Regarding the selection of scientific documents, we will select papers that have been published after December 2019 as then the new virus was identified. \n",
    "\n",
    "For this reason, since then all the articles are related to COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrYBr4_6cw7Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 10000\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Document_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        # we need a list of sentences\n",
    "        if isinstance(dataset, list): \n",
    "            self.dataset = dataset\n",
    "        else:\n",
    "            self.dataset = [dataset]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        return self.dataset[index]\n",
    "\n",
    "class CORD_Dataset_(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, limit=-1):\n",
    "        # dataset path\n",
    "        self.path = dataset_path\n",
    "        # a limit of elements to read\n",
    "        self.limit = limit\n",
    "        self.part_of_doc = 0\n",
    "\n",
    "        # open the csv file\n",
    "        df = pd.read_csv(self.path+\"/metadata.csv\")\n",
    "\n",
    "        \"\"\"\n",
    "        select documents after December of 2019\n",
    "        and also have a valid pdf json body text\n",
    "        \"\"\"\n",
    "        self.df = df[(df['publish_time'] > '2019-12-30') & (df['pdf_json_files'].notnull()) & (df['title'].notnull()) ]\n",
    "\n",
    "        # if limit = -1 then we need to find the true size of the dataset\n",
    "        if limit == -1:\n",
    "            # open the csv file and get the columns of cord_uid\n",
    "            self.limit = len(self.df[\"cord_uid\"])\n",
    "            # get the keys\n",
    "            self.keys = self.df[\"cord_uid\"].tolist()\n",
    "        else:\n",
    "            # get the keys\n",
    "            self.keys = self.df[\"cord_uid\"].tolist()[0:self.limit]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # len is the limit \n",
    "        return self.limit\n",
    "\n",
    "    def __getitem__(self, index):  \n",
    "\n",
    "        row = self.df.iloc[[index]]\n",
    "\n",
    "        if self.part_of_doc == 'title':\n",
    "            return Document_Dataset( [ row['title'].to_string(index=False) ] )\n",
    "\n",
    "        elif self.part_of_doc == 'authors':\n",
    "            return Document_Dataset( row['authors'].to_string(index=False).split('; ') )\n",
    "\n",
    "        elif self.part_of_doc == 'abstract':\n",
    "        # tokenize senteences of abstract\n",
    "            return Document_Dataset( nltk.sent_tokenize(row['abstract'].to_string(index=False)) )\n",
    "\n",
    "        elif self.part_of_doc == 'main_body':\n",
    "        # access the full text (if available) for Intro\n",
    "            main_body = []\n",
    "            if not row['pdf_json_files'].empty:\n",
    "                for json_path in row['pdf_json_files'].to_string(index=False).lstrip().split('; '):\n",
    "                    # print(json_path)\n",
    "                    with open(version+\"/\"+json_path) as f_json:\n",
    "                        full_text_dict = json.load(f_json)\n",
    "                        \n",
    "                        # grab main_body text section from *some* version of the full text\n",
    "                        # and tokenize to sentences\n",
    "                        for paragraph_dict in full_text_dict['body_text']:\n",
    "                            paragraph_text = paragraph_dict['text']\n",
    "                            # tokenize sentences with nltk\n",
    "                            sentence_text = nltk.sent_tokenize(paragraph_text) \n",
    "                            main_body.extend(sentence_text)\n",
    "\n",
    "                        # stop searching other copies of full text if already got main_body\n",
    "                        if main_body:\n",
    "                            break\n",
    "\n",
    "            return Document_Dataset( main_body )\n",
    "\n",
    "class CORD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, limit=-1):\n",
    "\n",
    "        # instantiate a dataset \n",
    "        self.dataset = CORD_Dataset_(version, limit)\n",
    "        self.limit = self.dataset.limit\n",
    "        self.keys = self.dataset.keys\n",
    "\n",
    "    def __len__(self):\n",
    "        # 4 parts of doc we have title, authors, abstract, main body\n",
    "        return 4\n",
    "\n",
    "    def __getitem__(self, part_of_doc):  \n",
    "        # set the part of doc for the CORD dataset\n",
    "        self.dataset.part_of_doc = part_of_doc    \n",
    "        return self.dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1z3faM88FRV"
   },
   "source": [
    "## Get the dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2VPnEFE8J9K",
    "outputId": "e8cbc380-4a71-4c8f-b1ba-33dbd27c3805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-05-26.tar.gz\n",
      "Unzipping dataset file\n",
      "Unzipping dataset document parses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "version = \"2020-05-26\"\n",
    "download_CORD19_dataset(version)\n",
    "dataset = CORD_Dataset(version, 5000)\n",
    "keys = dataset.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7rO2azYLnrC"
   },
   "source": [
    "## Functions to compute and save the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK1yCIFrIn5r"
   },
   "source": [
    "We want to feed all the sentences of our dataset to the model and then extract the outputs. Because the dataset is very large and google colab may interrupt the runtime we will save every time the embeddings to a unique file.\n",
    "This process will happen 3 times one for the title, one for the abstract, and one for the main body. The 3 separated files will be created which are going to be under the same directory, unique for every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjAniozZLiHs"
   },
   "source": [
    "\n",
    "First of all, we will create a list of datasets that are the same part of the document like title, abstract, and main body.\n",
    "\n",
    "Then we will pass this list to a function that finds the embeddings for every dataset and saves the tensor to a given directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2nBc6pELxnG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_embeddings(embeddings, directory, key, part_of_doc):\n",
    "    path = directory+\"/\"+key\n",
    "    file_name = part_of_doc+\".pt\"\n",
    "    # if directory doenst exist make one\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    # save torch array\n",
    "    torch.save(embeddings, os.path.join(path, file_name))\n",
    "\n",
    "\n",
    "\n",
    "def get_save_embeddings(model, tokenizer, dataset, keys, directory, part_of_doc):\n",
    "\n",
    "    # get the embeddings and save embeddings for every document in dataset\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        dataloader = DataLoader(dataset[i], batch_size=4, shuffle=False, pin_memory=True, num_workers=4, drop_last=False, collate_fn=tokenizer)\n",
    "\n",
    "        # if embeddings are already computed then dont compute them again\n",
    "        if not os.path.exists(directory+\"/\"+keys[i]+\"/\"+part_of_doc+\".pt\"):\n",
    "            print(keys[i], i)\n",
    "            t = time.time()\n",
    "            embeddings = get_embeddings(model, dataloader)\n",
    "            e = time.time()\n",
    "            save_embeddings(embeddings, directory, keys[i], part_of_doc)  \n",
    "            s = time.time()\n",
    "            print(e-t, s-e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpOtPspZj7x2"
   },
   "source": [
    "# 1st method pre-trained BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLHk6tZA2xoP"
   },
   "source": [
    "For our first method, we will feed our sentences to a BERT model [(Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) and use the output as word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnSzSnU82yMN"
   },
   "source": [
    "## Why BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fokzbb-c20OY"
   },
   "source": [
    "BERT is a method of pre-training language representations.\n",
    "It is a general-purpose \"language understanding\" model that can be used on tasks like question answering. BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP.\n",
    "\n",
    "Because the model is unsupervised it can be trained on enormous datasets from the internet like Wikipedia.\n",
    "\n",
    "BERT model can produce representations of a sentence that are content-free or contextual with unidirectionality or bidirectionality.\n",
    "The previous GloVe embeddings that we used didn't provide the context that we needed to produce meaningful embeddings.\n",
    "\n",
    "\n",
    "BERT was built based on the recent work of ELMo and ULMFit.\n",
    "These models were still but whether these models were unidirectional and shallowly bidirectional. The great advantage of BERT over these methods is that it uses transformers. Transformers use multi-head attention with feed-forward neural blocks. By multiplying the representations of each word to each word and get the inner product we can map a better relationship of their context combining with all the other words of the sentence. This method has been proven state-of-the-art and can produce better contextual embeddings. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS4vbWGX_0-w"
   },
   "source": [
    "### About the implemetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRYvWaBX0NYU"
   },
   "source": [
    "\n",
    "We will use the base pre-trained BERT model with 768 output layers. Also, we will take the average of all the outputs so that the new output will be an 768 dimension array of embeddings for each sentence. We will use the Uncased model because it's better unless we need to know case information which is important for tasks like (e.g., Named Entity Recognition or Part-of-Speech tagging).\n",
    "The part of downloading the pre-trained and tokenizing the words is done by the [huggingface library](https://huggingface.co/transformers/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzZ3XQuBqKBi"
   },
   "source": [
    "## Install the [huggingface library](https://huggingface.co/transformers/index.html) library and dowload the large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbIo3jzkqbri"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HMJaU4FPYNP"
   },
   "source": [
    "#### Checking for gpu *usage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSq2xsPMPXvR",
    "outputId": "86a39170-52d7-4901-c5a6-01c8b7da43a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# gpu for pytorch\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPLor6nr2oM1"
   },
   "source": [
    "### Get the pre-trained BERT model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VHbsEz52tqn"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model, dataloader):\n",
    "    # create an empty array sto store the ouputs of the model\n",
    "    # its size must be the size of the dataloader\n",
    "    data_array = torch.empty((len(dataloader.dataset), model.output_dim))\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        # Load the data to gpu\n",
    "        # t = time.time()\n",
    "        data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
    "        data[\"attention_mask\"] = data[\"attention_mask\"].to(device)\n",
    "        data[\"token_type_ids\"] = data[\"token_type_ids\"].to(device)\n",
    "        model_output = model(data).cpu()\n",
    "\n",
    "        # append to the data array\n",
    "        data_array[batch_idx*batch_size : (batch_idx*batch_size) + len(model_output)] = model_output \n",
    "        # print(time.time() - t)\n",
    "    # return the results from all the bathces\n",
    "    return  data_array\n",
    "\n",
    "class BERT_Model(torch.nn.Module):\n",
    "    def __init__(self, pre_trained, tokenizer):\n",
    "\n",
    "        super(BERT_Model, self).__init__()\n",
    "        self.bert = pre_trained\n",
    "        self.tokenizer = tokenizer\n",
    "        # base bert output\n",
    "        self.output_dim = self.bert.config.hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we average the last output layer of the pretrained bert model\n",
    "        \"\"\"\n",
    "        model_output = self.bert(**x).last_hidden_state.detach()\n",
    "        model_output = torch.mean(model_output, dim=1)\n",
    "        return model_output\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        \"\"\"\n",
    "        encode the given sentences\n",
    "        we do not use that during training in order to use the parallelization provided by the torch dataloader\n",
    "        \"\"\"\n",
    "        tokenizes_sentences = self.tokenizer(sentences)\n",
    "        tokenizes_sentences = tokenizes_sentences.to(device)\n",
    "        return self.forward(tokenizes_sentences)\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def BERT_tokenizer(batch):\n",
    "    return bert_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "BERT_model = BERT_Model(bert_model, BERT_tokenizer)\n",
    "BERT_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIeuCMR6iFQi"
   },
   "source": [
    "## Compute all the BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zBwQ7iGiJGw"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5066xMCJiUcx"
   },
   "outputs": [],
   "source": [
    "BERT_directory = \"/content/drive/MyDrive/BERT_embeddings\"\n",
    "\n",
    "part_of_doc = \"title\"\n",
    "\n",
    "get_save_embeddings(BERT_model, BERT_tokenizer,  dataset[part_of_doc], keys, BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYNdq3KEiKtp"
   },
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXdty3AJioJe"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"abstract\"\n",
    "\n",
    "get_save_embeddings(BERT_model, BERT_tokenizer,  dataset[part_of_doc], keys, BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhleABMuiSQc"
   },
   "source": [
    "### Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB9J0BxRio31"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"main_body\"\n",
    "\n",
    "get_save_embeddings(BERT_model, BERT_tokenizer,  dataset[part_of_doc], keys, BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVe3DfTWjiaW"
   },
   "source": [
    "### Zip embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3ThhfXdjmNT"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!zip -r \"/content/drive/MyDrive/BERT_embeddings.zip\" \"/content/drive/MyDrive/BERT_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSwVAndwdEmI",
    "outputId": "f14a2e82-7590-4a5f-a37b-c108a7abacc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4G\t/content/drive/MyDrive/BERT_embeddings\n"
     ]
    }
   ],
   "source": [
    "!du -sh  /content/drive/MyDrive/BERT_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtZ_NAHeyTBe",
    "outputId": "362a67ec-fc1c-4e3d-e411-44f734b37a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/BERT_embeddings | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYARqbTbOsKK"
   },
   "source": [
    "# 2nd method GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1mljTvLPKPh"
   },
   "source": [
    "## Why GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLIqdKSLPOan"
   },
   "source": [
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
    "\n",
    "Glove can create generalized representations for the words by taking into consideration the word's statistical occurrence in the text.\n",
    "\n",
    "These embeddings might not have a lot of contextual information as BERT's had but they are easy to compute and a lot faster to find a similar vector because it has less than half the dimension of BERT's vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgKqQvK-PO_X"
   },
   "source": [
    "### Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95i6Drl2QZlJ"
   },
   "source": [
    "As for the implementation, the embedding layer outputs a 300 dimension vector for every word that we throw into it. Storing 300*n values for every sentence is way over inefficient, so we will take the average and save only a 300 dimension vector for every sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ct2PLLePSlU"
   },
   "source": [
    "## Download the pre-trained glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CASOq-j6WtcD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hWgY7n0PZ01"
   },
   "source": [
    "### Functions that download the embeddings files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvGdMpHdQbUT"
   },
   "outputs": [],
   "source": [
    "# download and unzip embedding vectors file\n",
    "import json\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def download_embeddings(file):\n",
    "    if not os.path.exists(file+\".zip\"):\n",
    "        print(\"Download glove \"+file)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\"http://nlp.stanford.edu/data/\"+file+\".zip\", file+\".zip\")\n",
    "    else:\n",
    "        print(\"File already downloaded\")\n",
    "\n",
    "    if not os.path.exists(file+\".txt\"):\n",
    "        print(\"Unzip glove file\")\n",
    "        import zipfile\n",
    "        zip_ref = zipfile.ZipFile(file+\".zip\", 'r')\n",
    "        zip_ref.extractall()\n",
    "        zip_ref.close()\n",
    "    else:\n",
    "        print(\"File already unziped\")\n",
    "\n",
    "# get vectros arrays and ids of the vocabulary\n",
    "def get_embedings_vocab_id_array(file, embeddings_dim):\n",
    "\n",
    "    # check if embeddings dict is saved\n",
    "    if not os.path.exists(\"embeddings_dict.pickle\"):\n",
    "        # get embeddings dict\n",
    "        print(\"Getting embeddings dict\")\n",
    "        embeddings_dict = {}\n",
    "        with open(file+\".txt\", 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split(\" \")\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], np.float32)\n",
    "                embeddings_dict[word] = vector\n",
    "        with open('embeddings_dict.pickle', 'wb') as f:\n",
    "            pickle.dump(embeddings_dict, f)\n",
    "\n",
    "    else:\n",
    "        # if saved load it\n",
    "        print(\"Getting embeddings dict from pickle\")\n",
    "        with open('embeddings_dict.pickle', 'rb') as f:\n",
    "            embeddings_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "    # get vovabulary, id, embedings array\n",
    "    print(\"Getting vovabulary, id, embedings array\")\n",
    "    # set vector for unknown\n",
    "    embeddings_dict[\"<unk>\"] = np.random.normal(scale = 0.420, size=(embeddings_dim,  ))\n",
    "    # set vectori for pad\n",
    "    embeddings_dict[\"<pad>\"] = np.random.normal(scale = 0.69, size=(embeddings_dim,   ))\n",
    "    # vovabulary in set for faster indexing\n",
    "    vocabulary_words = set(embeddings_dict.keys())\n",
    "\n",
    "    # get the vocabulary id\n",
    "    vocabulary_words_id = {w:i for i,w in enumerate(vocabulary_words)}\n",
    "\n",
    "    # get the embeddings array to feed in the embeddings layer\n",
    "    embeddings_array = np.empty((len(vocabulary_words), embeddings_dim), dtype=np.float32)\n",
    "    for index, embedding_array in enumerate(embeddings_dict.values()):\n",
    "        embeddings_array[index] = embedding_array\n",
    "\n",
    "    return vocabulary_words, vocabulary_words_id, embeddings_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4FW94wXPubW"
   },
   "source": [
    "#### Download the pre-trained embeddigns for the glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq8TNzxWQcN2",
    "outputId": "ed3909ed-aed6-438b-c5fc-fef7df96c712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download glove glove.840B.300d\n",
      "Unzip glove file\n",
      "Getting embeddings dict\n",
      "Getting vovabulary, id, embedings array\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"glove.840B.300d\"\n",
    "embeddings_dim = 300\n",
    "download_embeddings(glove_file)\n",
    "vocabulary, vocabulary_id, embeddings_array = get_embedings_vocab_id_array(glove_file, embeddings_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v45TeaDqPgVz"
   },
   "source": [
    "#### Checking for GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHEljOQ9O4ka",
    "outputId": "0de8ea2b-0df2-4b59-ba93-3eb13b7b40a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# gpu for pytorch\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUlkqUKaVud_"
   },
   "source": [
    "### Get the pre-trained GLOVE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y8bc3qSSfRu",
    "outputId": "14ac6719-fb77-445a-a83d-87af22fc42de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLOVE_Model(\n",
       "  (embedding): Embedding(2196018, 300)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings(model, dataloader):\n",
    "    # create an empty array sto store the ouputs of the model\n",
    "    # its size must be the size of the dataloader\n",
    "    data_array = torch.empty((len(dataloader.dataset), model.output_dim))\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        # Load the data to gpu\n",
    "        # t = time.time()\n",
    "        data = data.to(device)\n",
    "        model_output = model(data).cpu()\n",
    "\n",
    "        # append to the data array\n",
    "        data_array[batch_idx*batch_size : (batch_idx*batch_size) + len(model_output)] = model_output \n",
    "        # print(time.time()-t)\n",
    "    # return the results from all the bathces\n",
    "    return  data_array\n",
    "\n",
    "class GLOVE_Model(torch.nn.Module):\n",
    "    def __init__(self, embeddings_array, tokenizer, trainable=False):\n",
    "        super(GLOVE_Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings_array))\n",
    "        self.embedding.weight.requires_grad = trainable\n",
    "        self.tokenizer = tokenizer\n",
    "        # fixed output size 300\n",
    "        self.output_dim = 300\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we average the output layer of the pretrained glove model\n",
    "        \"\"\"\n",
    "        model_output = self.embedding(x).detach()\n",
    "        model_output = torch.mean(model_output, dim=1)\n",
    "        return model_output\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        \"\"\"\n",
    "        encode the given sentences\n",
    "        we do not use that during training in order to use the parallelization provided by the torch dataloader\n",
    "        \"\"\"\n",
    "        tokenizes_sentences = self.tokenizer(sentences)\n",
    "        tokenizes_sentences = tokenizes_sentences.to(device)\n",
    "        return self.forward(tokenizes_sentences)\n",
    "\n",
    "def GLOVE_tokenizer(sentences, vocabulary_id=vocabulary_id, vocabulary=vocabulary):\n",
    "    # transform text to tokenized vocabulary ids\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tok_sentence = [vocabulary_id[w] if w in vocabulary else vocabulary_id[\"<unk>\"] for w in sentence.split()]\n",
    "        tokenized_sentences.append(tok_sentence)\n",
    "\n",
    "    # create a list of torch vectors\n",
    "    tokenized_vectors = [torch.tensor(vector) for vector in tokenized_sentences]\n",
    "    \n",
    "    # add padding\n",
    "    padded_tokenized_vectors = torch.nn.utils.rnn.pad_sequence(tokenized_vectors, padding_value=vocabulary_id[\"<pad>\"], batch_first=True)\n",
    "    return padded_tokenized_vectors\n",
    "\n",
    "GLOVE_model = GLOVE_Model(embeddings_array, GLOVE_tokenizer)\n",
    "GLOVE_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oidhxxQP3DR"
   },
   "source": [
    "## Compute all the GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmfn5DmxP6rk"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJGWBhOYP8D_"
   },
   "outputs": [],
   "source": [
    "GLOVE_directory = \"/content/drive/MyDrive/GLOVE_embeddings\"\n",
    "GLOVE_directory = \"GLOVE_embeddings_test\"\n",
    "\n",
    "part_of_doc = \"title\"\n",
    "\n",
    "get_save_embeddings(GLOVE_model, GLOVE_tokenizer,  dataset[part_of_doc], keys, GLOVE_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVYfbgbiP8tM"
   },
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd8_0CFhP_AC"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"abstract\"\n",
    "\n",
    "get_save_embeddings(GLOVE_model, GLOVE_tokenizer,  dataset[part_of_doc], keys, GLOVE_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re6DfHAOP_ki"
   },
   "source": [
    "### Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bf9uTeJCQBlP"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"main_body\"\n",
    "\n",
    "get_save_embeddings(GLOVE_model, GLOVE_tokenizer,  dataset[part_of_doc], keys, GLOVE_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5I3K3EJjxjx"
   },
   "source": [
    "### Zip embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIKCUKKWjxjy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!zip -r \"/content/drive/MyDrive/GLOVE_embeddings.zip\" \"/content/drive/MyDrive/GLOVE_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0qRkX_fdOss",
    "outputId": "ef1e23d7-9051-4228-9b40-30c1eb8203d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971M\t/content/drive/MyDrive/GLOVE_embeddings\n"
     ]
    }
   ],
   "source": [
    "!du -sh  /content/drive/MyDrive/GLOVE_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjot0FIkphyF",
    "outputId": "9579dd49-e4a5-4338-d87d-58e490e5efb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/GLOVE_embeddings | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b26u-G7T4ghM"
   },
   "source": [
    "# 3rd method pretrained BERT large model, on a corpus of messages from Twitter about COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k81SrYcn487T"
   },
   "source": [
    "## About the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpIZaI9Z5ADW"
   },
   "source": [
    "COVID-Twitter-BERT (CT-BERT) is a transformer-based model pretrained on a large corpus of Twitter messages on the topic of COVID-19. The v2 model is trained on 97M tweets (1.2B training examples).\n",
    "\n",
    "When used on domain specific datasets our evaluation shows that this model will get a marginal performance increase of 10–30% compared to the standard BERT-Large-model. Most improvements are shown on COVID-19 related and on Twitter-like messages.\n",
    "\n",
    "The github page of this model can be found [here](https://github.com/digitalepidemiologylab/covid-twitter-bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGScGMw_61sc"
   },
   "source": [
    "Using a trained model on texts related to the coronavirus we can have a better understanding of our sentences as it will be known specific words that are only related to it. For example the text that has already been trained in BERT words like coronavirus are quite rare. Therefore, by using this ready-made model on these topics, we will have a better semantic representation of the propositions in vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyL97SQd66TP"
   },
   "source": [
    "As for the implementation, we will use the hugging face library. This model has been uploaded to the huggingface repository and we can download it from there. After that, we will follow the same steps as we followed using the BERT embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF3sBuw9ChIX"
   },
   "source": [
    "## Install the [huggingface library](https://huggingface.co/transformers/index.html) library and dowload the large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rdfodnfChIa"
   },
   "source": [
    "We will use the Uncased model because it's better unless we need to know case information which is important for tasks like (e.g., Named Entity Recognition or Part-of-Speech tagging).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcIbxhL8ChIc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bek-jgGPChId"
   },
   "source": [
    "#### Checking for gpu *usage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhPYGAlMChIe",
    "outputId": "425a84a1-8a77-4a27-934d-b19823aeea42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# gpu for pytorch\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0LivztsChIg"
   },
   "source": [
    "### Get the pre-trained BERT model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME6nnSjJChIg"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model, dataloader):\n",
    "    # create an empty array sto store the ouputs of the model\n",
    "    # its size must be the size of the dataloader\n",
    "    data_array = torch.empty((len(dataloader.dataset), model.output_dim))\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        # Load the data to gpu\n",
    "        # t = time.time()\n",
    "        data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
    "        data[\"attention_mask\"] = data[\"attention_mask\"].to(device)\n",
    "        data[\"token_type_ids\"] = data[\"token_type_ids\"].to(device)\n",
    "        model_output = model(data).cpu()\n",
    "\n",
    "        # append to the data array\n",
    "        data_array[batch_idx*batch_size : (batch_idx*batch_size) + len(model_output)] = model_output \n",
    "        # print(time.time() - t)\n",
    "    # return the results from all the bathces\n",
    "    return  data_array\n",
    "\n",
    "class BERT_Model(torch.nn.Module):\n",
    "    def __init__(self, pre_trained, tokenizer):\n",
    "\n",
    "        super(BERT_Model, self).__init__()\n",
    "        self.bert = pre_trained\n",
    "        self.tokenizer = tokenizer\n",
    "        # base bert output\n",
    "        self.output_dim = self.bert.config.hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we average the last output layer of the pretrained bert model\n",
    "        \"\"\"\n",
    "        model_output = self.bert(input_ids=x[\"input_ids\"], attention_mask=x[\"attention_mask\"] ).last_hidden_state.detach()\n",
    "        model_output = torch.mean(model_output, dim=1)\n",
    "        return model_output\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        \"\"\"\n",
    "        encode the given sentences\n",
    "        we do not use that during training in order to use the parallelization provided by the torch dataloader\n",
    "        \"\"\"\n",
    "        tokenizes_sentences = self.tokenizer(sentences)\n",
    "        tokenizes_sentences = tokenizes_sentences.to(device)\n",
    "        return self.forward(tokenizes_sentences)\n",
    "\n",
    "\n",
    "twitter_bert_tokenizer = BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2', model_max_length=512)\n",
    "twitter_bert_model = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')\n",
    "\n",
    "def COVID_TWITER_BERT_tokenizer(batch):\n",
    "    return twitter_bert_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "COVID_TWITER_BERT_model = BERT_Model(twitter_bert_model, COVID_TWITER_BERT_tokenizer)\n",
    "COVID_TWITER_BERT_model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izt9z53PChIi"
   },
   "source": [
    "## Compute all the BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OkGgyLUChIj"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UdfE8_9ChIk"
   },
   "outputs": [],
   "source": [
    "COVID_TWITER_BERT_directory = \"/content/drive/MyDrive/COVID_TWITER_BERT_embeddings\"\n",
    "\n",
    "part_of_doc = \"title\"\n",
    "\n",
    "get_save_embeddings(COVID_TWITER_BERT_model, COVID_TWITER_BERT_tokenizer,  dataset[part_of_doc], keys, COVID_TWITER_BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo08Sf-aChIl"
   },
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_E5mAecChIl"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"abstract\"\n",
    "\n",
    "get_save_embeddings(COVID_TWITER_BERT_model, COVID_TWITER_BERT_tokenizer,  dataset[part_of_doc], keys, COVID_TWITER_BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK-y_GO3ChIm"
   },
   "source": [
    "### Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpO7RladChIm"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"main_body\"\n",
    "\n",
    "get_save_embeddings(COVID_TWITER_BERT_model, COVID_TWITER_BERT_tokenizer,  dataset[part_of_doc], keys, COVID_TWITER_BERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhqluNwMChIm"
   },
   "source": [
    "### Zip embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tp86VwqeChIn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# w81ysjf9\n",
    "!zip -r \"/content/drive/MyDrive/COVID_TWITER_BERT_embeddings.zip\" \"/content/drive/MyDrive/COVID_TWITER_BERT_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eA2LSqBOChIn",
    "outputId": "97238529-df6c-44e2-cc32-ad3db1017787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2G\t/content/drive/MyDrive/COVID_TWITER_BERT_embeddings\n"
     ]
    }
   ],
   "source": [
    "!du -sh  /content/drive/MyDrive/COVID_TWITER_BERT_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDWZxWecChIn",
    "outputId": "0c896097-ce6f-4533-e946-88dee6090ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/COVID_TWITER_BERT_embeddings | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2QzFFTpExWf"
   },
   "source": [
    "# 4th method pre-trained Sentence-BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "039tVPcjExWg"
   },
   "source": [
    "For our last method, we will feed our sentences to a Sentence-BERT model [(Nils Reimers and Iryna Gurevych, 2019)](https://arxiv.org/pdf/1908.10084.pdf) and use the output as word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCN1ZCf_ExWi"
   },
   "source": [
    "## Why Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps1hG-_TExWj"
   },
   "source": [
    "Sentence-BERT(SBERT) as described by the paper of [(Nils Reimers and Iryna Gurevych, 2019)](https://arxiv.org/pdf/1908.10084.pdf) is a  modification of the pre-trained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. \n",
    "\n",
    "BERT produces semantically meaningful embeddings but it had never been trained to do that exact thing. It was trained as a language model and a model that could predict if a sentence is semantically relevant to another. \n",
    "\n",
    "SBERT takes that advantage of BERT and it is trained to a labeled dataset in order to fine-tune its ability to produce quality embeddings.\n",
    "\n",
    "In order to fine-tune BERT the authors of the paper created siamese and triplet networks and updated the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n",
    "\n",
    "This method beat the average embeddings of BERT in the STS score and also the Glove average embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-bOU8zSExWj"
   },
   "source": [
    "### About the implemetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPhh1wLExWk"
   },
   "source": [
    "\n",
    "We will use the base pre-trained S-BERT model with 768 output layers. Also, we will take the average of all the outputs so that the new output will be an 768 dimension array of embeddings for each sentence.\n",
    "\n",
    "However the sentence transformers library can do this job for us and we will be using it to get the embeddings from the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EnnQG4dExWl"
   },
   "source": [
    "## Install the [sentence transformers](https://github.com/UKPLab/sentence-transformers) library and dowload the base distil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvTZre_UExWn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence-transformers --upgrade\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkHihanzExWn"
   },
   "source": [
    "#### Checking for gpu *usage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u04YKxtRExWo",
    "outputId": "7081bbba-a130-4bff-85fb-3d855e2fd535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# gpu for pytorch\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIbF3VL8ExWr"
   },
   "source": [
    "### Get the pre-trained S-BERT model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJVx-znvExWr"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model, dataloader):\n",
    "    # create an empty array sto store the ouputs of the model\n",
    "    # its size must be the size of the dataloader\n",
    "    data_array = torch.empty((len(dataloader.dataset), 768))\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        # Load the data to gpu\n",
    "        # t = time.time()\n",
    "\n",
    "        model_output = model.encode(data)\n",
    "\n",
    "        # append to the data array\n",
    "        data_array[batch_idx*batch_size : (batch_idx*batch_size) + len(model_output)] = torch.from_numpy(model_output)\n",
    "        # print(time.time() - t)\n",
    "        break\n",
    "    # return the results from all the bathces\n",
    "    return  data_array\n",
    "\n",
    "\n",
    "SBERT_model = SentenceTransformer('stsb-distilbert-base')\n",
    "\n",
    "# no need for special tokenizer\n",
    "def SBERT_tokenizer(batch):\n",
    "    return batch\n",
    "\n",
    "SBERT_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef721cb_ExWt"
   },
   "source": [
    "## Compute all the SBERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwae5quLExWt"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAz9YJ7nExWu"
   },
   "outputs": [],
   "source": [
    "SBERT_directory = \"/content/drive/MyDrive/SBERT_embeddings\"\n",
    "\n",
    "part_of_doc = \"title\"\n",
    "\n",
    "get_save_embeddings(SBERT_model, SBERT_tokenizer,  dataset[part_of_doc], keys, SBERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDexcd8jExWu"
   },
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVL0q9qjExWu"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"abstract\"\n",
    "\n",
    "get_save_embeddings(SBERT_model, SBERT_tokenizer,  dataset[part_of_doc], keys, SBERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Le29SjIExWv"
   },
   "source": [
    "### Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEgwQpA9ExWv"
   },
   "outputs": [],
   "source": [
    "part_of_doc = \"main_body\"\n",
    "\n",
    "get_save_embeddings(SBERT_model, SBERT_tokenizer,  dataset[part_of_doc], keys, SBERT_directory, part_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTwndg1GExWv"
   },
   "source": [
    "### Zip embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaFXXcFWExWw"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!zip -r \"/content/drive/MyDrive/SBERT_embeddings.zip\" \"/content/drive/MyDrive/SBERT_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilBRmGUFExWw",
    "outputId": "bd44e89f-db93-41e0-cf83-a872f2fd7976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4G\t/content/drive/MyDrive/SBERT_embeddings\n"
     ]
    }
   ],
   "source": [
    "!du -sh  /content/drive/MyDrive/SBERT_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyP93UPBExWx",
    "outputId": "c6c7af7e-174f-4248-f481-544679eb41d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/SBERT_embeddings | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEL-y3jUC6Kf"
   },
   "source": [
    "# About the measure of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpUclAIjWzuQ"
   },
   "source": [
    "To calculate the similarity we will calculate the cosine similarity for each sentence from each scientific document.\n",
    "\n",
    "We have initially saved all the representation vectors of the sentences from each article. Then we scan each vector and measure the similarity with the question. Finally we will find the sentence with the greatest similarity and view the document that had this sentence in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-l0Ws4dH8M_"
   },
   "source": [
    "## Methods that get the saved embeddings and load them in a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8pNEH1KII3X"
   },
   "source": [
    "With the methods below, we load the embeddings from a given directory to associate them with the given queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHqEN8dsIGLN"
   },
   "outputs": [],
   "source": [
    "def extract_saved_doc_embeddings(directory, key, part_of_document=[\"title\", \"abstract\", \"main_body\"]):\n",
    "    # find the embeddings save torch files\n",
    "    path = directory+\"/\"+key\n",
    "    files = []\n",
    "    part_of_document = [p+\".pt\" for p in part_of_document]\n",
    "    paths = os.listdir(path)\n",
    "    # add title\n",
    "    for file in paths:\n",
    "        if file.endswith(\".pt\") and (file in part_of_document) and file==\"title.pt\":\n",
    "            files.append(os.path.join(path, file))\n",
    "    # add abstract\n",
    "    for file in paths:\n",
    "        if file.endswith(\".pt\") and (file in part_of_document) and file==\"abstract.pt\":\n",
    "            files.append(os.path.join(path, file))\n",
    "    # add main body\n",
    "    for file in paths:\n",
    "        if file.endswith(\".pt\") and (file in part_of_document) and file==\"main_body.pt\":\n",
    "            files.append(os.path.join(path, file))\n",
    "\n",
    "\n",
    "    # open the torch files and concat all in one array\n",
    "    embedding_arrays = []\n",
    "    for embedding_array in files:\n",
    "        embedding_arrays.append( torch.load(embedding_array) )\n",
    "\n",
    "    # concat all the tensors\n",
    "    document_embedding_arrays = torch.cat( embedding_arrays, 0)\n",
    "    return document_embedding_arrays\n",
    "\n",
    "\n",
    "\n",
    "class Saved_Embeddings_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, keys, part_of_document=[\"title\", \"abstract\", \"main_body\"]):\n",
    "        self.directory = directory\n",
    "        self.keys = keys\n",
    "        self.part_of_document = part_of_document \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        return extract_saved_doc_embeddings(self.directory, self.keys[index], self.part_of_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emQP7Om4Ihma"
   },
   "source": [
    "## Similarity Comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc_3iLbg-cNQ"
   },
   "source": [
    "Cosine similarity is a simple modifier to compare the similarity between 2 vectors. We could use other ways to find the similarity such as running the BERT model but this would be very time consuming and costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmLTtX4QItOs"
   },
   "outputs": [],
   "source": [
    "class Cosine_Similarity_Comparator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Cosine_Similarity_Comparator, self).__init__()\n",
    "        self.CosineSimilarity = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        self.indexes_passed = 0\n",
    "        self.max_index_found_at = [0, 0]\n",
    "        self.max_similarity = torch.tensor(-1)\n",
    "\n",
    "    def forward(self, question, data):\n",
    "\n",
    "        similarity = self.CosineSimilarity(question.reshape(1,-1), data)\n",
    "\n",
    "        temp_max = similarity.max()\n",
    "        temp_argmax = torch.argmax(similarity)\n",
    "        if temp_max > self.max_similarity:\n",
    "            self.max_index_found_at = self.indexes_passed, temp_argmax\n",
    "            self.max_similarity = temp_max\n",
    "        # print(self.max)\n",
    "        self.indexes_passed += 1\n",
    "\n",
    "# scan all the embeddings dataset and find the sentences with the highest score\n",
    "def get_most_similar_document_index(question, model, dataset, comparator=Cosine_Similarity_Comparator()):\n",
    "\n",
    "    # get a loader of size 1 to scan one document each time\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=4, drop_last=False)\n",
    "\n",
    "    # start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # encode the question with embeddigns\n",
    "    encoded_question = model.encode([question])\n",
    "    if not isinstance(encoded_question, torch.Tensor):\n",
    "        encoded_question = torch.from_numpy(encoded_question)    \n",
    "    encoded_question = encoded_question.to(device)\n",
    "\n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        # tim = time.time()\n",
    "        # pass data to device if gpu is available\n",
    "        data = data.to(device)\n",
    "        # comporator keeps the results\n",
    "        comparator(encoded_question, data)\n",
    "        # print(time.time() - tim, batch_idx)\n",
    "\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "\n",
    "    # and returns the indexe of the best one\n",
    "    return comparator.max_index_found_at, (comparator.max_similarity, query_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzkzamoUKYof"
   },
   "source": [
    "# Let's ask some questions on both embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WCGaIVgIvGj"
   },
   "source": [
    "## Methods that print the title and the relevant passages of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EUW21ZQKSqZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_similar_title_and_document(indexes, dataset, similarity, window=2):\n",
    "\n",
    "    document_index = indexes[0]\n",
    "    print(\"Document found with similarity [%f] in time [%f] seconds\"% similarity)\n",
    "    title = dataset[\"title\"][document_index][:][0]\n",
    "    print(\"Title\\n{\"+title+\"}\")\n",
    "\n",
    "    # lets find from the index the sentence in the document\n",
    "    # get document sentences by summing all the document part sentences into one\n",
    "    sentence_document = [title]\n",
    "    sentence_document.extend(dataset[\"abstract\"][document_index][:])\n",
    "    sentence_document.extend(dataset[\"main_body\"][document_index][:])\n",
    "\n",
    "    index = indexes[1]\n",
    "    # print a small window before and after the most similar sentence\n",
    "    print(\"Main body\")\n",
    "    for w in reversed(range(window)):\n",
    "        if index-w-1 > 0:\n",
    "            print(sentence_document[index-w-1])\n",
    "    print(\">>>{\"+sentence_document[index]+\"}<<<\")\n",
    "    for w in range(window):\n",
    "        if index+w+1 < len(sentence_document):\n",
    "            print(sentence_document[index+w+1])\n",
    "\n",
    "def get_similar_title_and_document(question, model, dataset, embeddings_dataset):\n",
    "    comparator = Cosine_Similarity_Comparator()\n",
    "    comparator.to(device)\n",
    "    model.to(device)\n",
    "    indexes, similarity = get_most_similar_document_index(question, model, embeddings_dataset, comparator)\n",
    "    print_similar_title_and_document(indexes, dataset, similarity)\n",
    "\n",
    "question = \"I don't like coronovirus\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctli88GYK7Xp"
   },
   "source": [
    "## Load the embeddings datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATPx4ctmj6Wo"
   },
   "source": [
    "Transfer the ziped embeddings to the local storage space and unzip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr2D9TWkkEB8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!cp -R /content/drive/MyDrive/BERT_embeddings.zip ./\n",
    "!cp -R /content/drive/MyDrive/GLOVE_embeddings.zip ./\n",
    "!cp -R /content/drive/MyDrive/COVID_TWITER_BERT_embeddings.zip ./\n",
    "!cp -R /content/drive/MyDrive/SBERT_embeddings.zip ./\n",
    "\n",
    "\n",
    "!unzip \"BERT_embeddings.zip\" \n",
    "!unzip \"GLOVE_embeddings.zip\" \n",
    "!unzip \"COVID_TWITER_BERT_embeddings.zip\" \n",
    "!unzip \"SBERT_embeddings.zip\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1p_eVGMK7F-"
   },
   "outputs": [],
   "source": [
    "BERT_directory = \"content/drive/MyDrive/BERT_embeddings\"\n",
    "GLOVE_directory = \"content/drive/MyDrive/GLOVE_embeddings\"\n",
    "COVID_TWITER_BERT_directory = \"content/drive/MyDrive/COVID_TWITER_BERT_embeddings\"\n",
    "SBERT_directory = \"content/drive/MyDrive/SBERT_embeddings\"\n",
    "\n",
    "\n",
    "BERT_embeddings_dataset = Saved_Embeddings_Dataset(BERT_directory, keys)\n",
    "GLOVE_embeddings_dataset = Saved_Embeddings_Dataset(GLOVE_directory, keys)\n",
    "COVID_TWITER_BERT_embeddings_dataset = Saved_Embeddings_Dataset(COVID_TWITER_BERT_directory, keys)\n",
    "SBERT_embeddings_dataset = Saved_Embeddings_Dataset(SBERT_directory, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa1f4utMLVBC"
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIFVDE__LWmH"
   },
   "source": [
    "### What are the coronoviruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thj2LJPoLhBA",
    "outputId": "b05f4302-c87f-4e37-94be-8f4920dc68a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.787640] in time [70.818873] seconds\n",
      "Title\n",
      "{ Teschovirus}\n",
      "Main body\n",
      ">>>{ Teschoviruses are emerging pathogens, belonging to the family Picornaviridae, and infects porcine population only.}<<<\n",
      "Among all, porcine teschoviruses (PTVs) are of high prominence leading to clinical illness and consequent economic loss to the livestock sector.\n",
      "These are associated with extremely lethal non-suppurative polioencephalomyelitis (Teschen disease) and are distributed world over.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.657842] in time [28.851360] seconds\n",
      "Title\n",
      "{ The biomechanical role of extra-axonemal structures in shaping the flagellar beat of Euglena}\n",
      "Main body\n",
      "Since H 1 = 0 we must have (c 1 , c 2 ) = (0, 0).\n",
      "However, in this case, (24) admits the unique solution U = 0, which is incompatible with (25) .\n",
      ">>>{If H 1 = 0 then the boundary conditions impose c 1 = 0, but in this case (24) has again U = 0 as a unique solution, which is incompatible with both the boundary conditions and with (25) .}<<<\n",
      "Our statement is thus proved.\n",
      "For φ p ≈ −2π/9, the characteristic value for Euglena, the non-planarity of flagellar shapes is not just possible.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.607162] in time [94.840230] seconds\n",
      "Title\n",
      "{ COVID-19 in Iran, a comprehensive investigation from exposure to treatment outcomes}\n",
      "Main body\n",
      "Hydroxychloroquine and lopinavir/ritonavir (in younger age group) can be potential treatment options.\n",
      "Finally, patients discharged from the hospital should be followed up because of potential symptom relapse.\n",
      ">>>{Coronaviruses are the second cause of the common cold after rhinoviruses.}<<<\n",
      "1 19) has stricken the global health and the economy even more than the previous ones.\n",
      "It has spread to more than 209 countries/territories and has infected more than a million people around the world.\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.625057] in time [70.801620] seconds\n",
      "Title\n",
      "{ Oxidative Stress in Canine Histiocytic Sarcoma Cells Induced by an Infection with Canine Distemper Virus Led to a Dysregulation of HIF-1α Downstream Pathway Resulting in a Reduced Expression of VEGF-B In Vitro}\n",
      "Main body\n",
      "In non-infected DH82 cells, HIF-1α was mainly expressed within nucleus (median = 43.69%, range: 4.76%-69.49%) and cytoplasm (median = 30.38%, range: 20.31%-95.24%) and only to a lesser extent in the membrane (median: 20.75%, range: 0.00%-35.94%), without significant differences (p ranging from 0.1980 to >0.9999) between the three localizations.\n",
      "Interestingly, DH82Ond pi cells displayed a significantly higher HIF-1α expression in the membrane ( Figure 3 ) compared to nuclear (p = 0.0486; membrane median = 64.74%, membrane range: 22.80%-85.02%; nuclear median = 14.06%, nuclear range: 4.20%-29.05%) but not to cytoplasmic localizations (p = 0.0710; cytoplasm median = 21.01%, cytoplasm range: 10.78%-25.58%).\n",
      ">>>{Additionally, the membranous immunopositivity for HIF-1α in DH82Ond pi cells was significantly (p = 0.0317) higher when compared to the corresponding localization in non-infected Despite a lack of difference in ROS-induced nucleic acid damage as determined by immunofluorescence of 8OHdG, these results are collectively indicative of an increased oxidative stress in DH82Ond pi cells compared to non-infected DH82 cells, which might lead to an increased level of HIF-1α and subsequently to an inhibition of its degradation.}<<<\n",
      "Among the gene symbols referring to the functional group \"HIF-1α activation, transcriptional activity and regulation\", three out of 15 genes were down-regulated (Table 2) .\n",
      "Specifically, down-regulated gene symbols were those referring to two (ENGL1 and ENGL3) out of three prolyl hydroxylases and to von Hippel-Lindau (VHL) protein, while HIF-1α gene symbol (HIF1A) did not show any significant change (Supplementary Table S2) .\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the coronoviruses\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkeboNTMNMXb"
   },
   "source": [
    "### What was discovered in Wuhuan in December 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEY0_xBUNipc",
    "outputId": "077d1ff7-2062-44de-b163-1a316d1a319e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.791220] in time [70.831984] seconds\n",
      "Title\n",
      "{ Social distance and SARS memory: impact on the public awareness of 2019 novel coronavirus (COVID-19) outbreak}\n",
      "Main body\n",
      "Xilingol League in Inner Mongolia ranked 4 th , with a retention rate at 103%.\n",
      "Xilingol is far away from Wuhan in terms of social distance, but it was struck by SARS.\n",
      ">>>{It is worth noting that a confirmed case of plague was reported in Xilingol on Nov 16 th , 2019, only 45 days before the Wuhan outbreak.}<<<\n",
      "The effects of social distance and SARS memory on the lead-time advantage are estimated according to Eq.\n",
      "4, controlled by Euclidean distances, GDP per capita and the city's administrative level (Table 1) .\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.638674] in time [28.845056] seconds\n",
      "Title\n",
      "{ Human antibodies neutralizing diphtheria toxin in vitro and in vivo}\n",
      "Main body\n",
      "Unbound antibodies were removed by additional washing steps.\n",
      "Bound antibodies were visualized with TMB substrate (20 parts TMB solution A and 1 part TMB solution B).\n",
      ">>>{After stopping the reaction by adding 100 µL 1 N H 2 SO 4 , absorbance at 450 nm with a 620 nm reference was measured in an ELISA plate reader.}<<<\n",
      "pipe cloning.\n",
      "Polymerase incomplete primer extension (PIPE) cloning was used to introduce a site directed mutagenesis followed by cloning into the pCSE2.7-hIgG1-XP vector.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.617636] in time [94.825666] seconds\n",
      "Title\n",
      "{ Covid-19 Detection using CNN Transfer Learning from X-ray Images}\n",
      "Main body\n",
      ">>>{ The Covid-19 first occurs in Wuhan, China in December 2019.}<<<\n",
      "After that the virus spread all around the world and at the time of writing this paper the total number of confirmed cases are above 4 million with over 297000 deaths.\n",
      "Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process.\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.853732] in time [71.000834] seconds\n",
      "Title\n",
      "{ A systematic review to evaluate the clinical outcomes in COVID -19 patients on angiotensin converting enzyme inhibitors or angiotensin receptor blockers}\n",
      "Main body\n",
      "Additionally, the individual patient factors like ACE2 polymorphisms which might confer higher risk of adverse outcomes need to be evaluated further.\n",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-COV2) causes coronavirus disease , a potentially fatal disease that is of immense global public health concern.\n",
      ">>>{The initial cases were reported in December 2019 in Wuhan, China [1] .}<<<\n",
      "Since then, there have been 3,041,764 confirmed COVID-19 patients in the world as of April 27th ,2020 with a total of 211,167 deaths.\n",
      "The United States has the maximum number (988,189) of confirmed cases with a total of 56,259 deaths.\n"
     ]
    }
   ],
   "source": [
    "question = \"What was discovered in Wuhuan in December 2019\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lrQBzH6NQ3b"
   },
   "source": [
    "### What is Coronovirus Disease 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bskDwXC2NjRj",
    "outputId": "63a03b24-5924-4c4e-d3c3-fb65eccdf1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.801401] in time [70.817739] seconds\n",
      "Title\n",
      "{ Performing Structural Heart Disease Interventions During the Coronavirus Disease 2019 (COVID-19) Pandemic – But What Are the Downsides?}\n",
      "Main body\n",
      ">>>{ Performing Structural Heart Disease Interventions During the Coronavirus Disease 2019 (COVID-19) Pandemic – But What Are the Downsides?}<<<\n",
      " NaN\n",
      "Angiography and Interventions consensus statement on triage considerations for patients referred for structural heart disease (SHD) intervention during the current coronavirus disease 2019 (COVID-19) pandemic by Shah et al (1) .\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.677447] in time [28.797335] seconds\n",
      "Title\n",
      "{ Method for Active Pandemic Curve Management (MAPCM)}\n",
      "Main body\n",
      "These numbers are guesstimates, but can be replaced by reliable data.\n",
      "There are conflicting reports about fatality rates.\n",
      ">>>{While low tests number in combination with deaths as a percentage of confirmed cases result in a very low death percentage, it is not the case for percentage deaths among resolved cases, as discussed on page 9 the fatality rate among resolved cases is significantly higher.}<<<\n",
      "The number of infected include symptomatic and asymptomatic infections in the USA.\n",
      "Test efficiency is the percentage of people who became symptomatic after the average incubation period who've tested positive for COVID-19.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.655674] in time [94.904026] seconds\n",
      "Title\n",
      "{ Two mechanisms for accelerated diffusion of COVID-19 outbreaks in regions with high intensity of population and polluting industrialization: the air pollution-to-human and human-to-human transmission dynamics}\n",
      "Main body\n",
      ">>>{ What is COVID-19?}<<<\n",
      "Coronavirus disease 2019 (COVID-19) is viral infection that generates a severe acute respiratory syndrome with serious pneumonia that may result in progressive respiratory failure and death.\n",
      "What are the goals of this investigation?\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.735789] in time [70.790148] seconds\n",
      "Title\n",
      "{ Transmission dynamics of 2019 novel coronavirus (2019-nCoV)}\n",
      "Main body\n",
      ">>>{ Transmission dynamics of 2019 novel coronavirus (2019-nCoV)}<<<\n",
      " Background Since December 29, 2019, pneumonia infection with 2019-nCoV has rapidly spread out from Wuhan, Hubei Province, China to most others provinces and other counties.\n",
      "However, the transmission dynamics of 2019-nCoV remain unclear.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Coronovirus Disease 2019\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnLLgdw-NTUP"
   },
   "source": [
    "### What is COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yMNjC6WNjxl",
    "outputId": "9952239b-5f96-421e-f413-845203a23a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.830035] in time [70.819024] seconds\n",
      "Title\n",
      "{ All about COVID-19 in brief}\n",
      "Main body\n",
      ">>>{ All about COVID-19 in brief}<<<\n",
      " NaN\n",
      "A new coronavirus was discovered due to detection of an unfamiliar pneumonia in a group of patients in December 2019 in Wuhan, China, initially named as 2019 novel coronavirus (2019-nCoV) by the World Health Organization (WHO) on 7 January.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.621421] in time [28.818199] seconds\n",
      "Title\n",
      "{ Evaluation of \"stratify and shield\" as a policy option for ending the COVID-19 lockdown in the UK}\n",
      "Main body\n",
      "The 196 sensitivity (1 − p 1 ) of the classifier, with threshold set so that 15% of the population 197 will be classified as high risk, is the maximal proportion of deaths that can be prevented 198 by a stratify-and shield-policy optimally applied, in comparison with an unselective 199 lifting of social distancing.\n",
      "The results show that even under unfavourable assumptions 200 -a classifier of modest performance and an infection fatality ratio as high as 0.4% -the 201 sensitivity of the classifier is at least 75%.\n",
      ">>>{If the population-wide infection fatality ratio is as low as 0.1%, then even with a 203 classifier using that provides 3 bits of information for discrimination, equivalent to using 204 only age and sex, the mortality risk in unshielded susceptible individuals is less than 1 205 in 5000, though the total expected deaths in unshielded individuals is about 11,000 in If however the infection fatality ratio is as high as 0.15%, then a classifier of higher 212}<<<\n",
      "April 25, 2020 6/12 .\n",
      "CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.778703] in time [95.048658] seconds\n",
      "Title\n",
      "{ Laboratory Diagnosis of Novel Coronavirus Disease 2019 (COVID-19) Infection}\n",
      "Main body\n",
      "Also in lowand middle-income countries (LMIC) (Hopman et al.\n",
      "2020 ), the healthcare system is not robust enough as a result of which the testing laboratories often face difficulties in the performance of molecular testing.\n",
      ">>>{• COVID-19}<<<\n",
      "-Severe acute respiratory infection caused by novel coronavirus SARS-CoV-2.\n",
      "-First reported from Wuhan, China, as a cluster of cases with pneumonia.\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.961446] in time [70.815433] seconds\n",
      "Title\n",
      "{ Two mechanisms for accelerated diffusion of COVID-19 outbreaks in regions with high intensity of population and polluting industrialization: the air pollution-to-human and human-to-human transmission dynamics}\n",
      "Main body\n",
      ">>>{ What is COVID-19?}<<<\n",
      "Coronavirus disease 2019 (COVID-19) is viral infection that generates a severe acute respiratory syndrome with serious pneumonia that may result in progressive respiratory failure and death.\n",
      "What are the goals of this investigation?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is COVID-19\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vafBOl4LNWgK"
   },
   "source": [
    "### What is caused by SARS-COV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y543NMqoNkdW",
    "outputId": "a4c2aeaf-5257-4ef8-eb9b-cd929f11071b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.877689] in time [70.867692] seconds\n",
      "Title\n",
      "{ Patient-derived mutations impact pathogenicity of SARS-CoV-2}\n",
      "Main body\n",
      ">>>{ Patient-derived mutations impact pathogenicity of SARS-CoV-2}<<<\n",
      " The sudden outbreak of the severe acute respiratory syndrome-coronavirus (SARS-CoV-2) has spread globally with more than 1,300,000 patients diagnosed and a death toll of 70,000.\n",
      "Current genomic survey data suggest that single nucleotide variants (SNVs) are abundant.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.666858] in time [28.800757] seconds\n",
      "Title\n",
      "{ MATHEMATICAL MODELING FOR TRANSMISSIBILITY OF COVID-19 VIA MOTORCYCLES}\n",
      "Main body\n",
      "https://doi.org/10.1101/2020.04.\n",
      "18.20070797 doi: medRxiv preprint Case II.2 .\n",
      ">>>{If there exists a natural number k : a k = 0, then the solution of Equation 3.11 is zero for t > t k , i.e., all solutions in spite of the initial value x 0 coincide for t > t k .}<<<\n",
      "Case II.3 .\n",
      "If a k = e −k , t k = k, k = 1, 2, .\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.790164] in time [94.926750] seconds\n",
      "Title\n",
      "{ The third coronavirus epidemic in the third millennium: what’s next?}\n",
      "Main body\n",
      "Overall, 19.1% of all MERS cases have been among health care workers, and more than half of all laboratory-confirmed secondary cases were transmitted from human to human in health care settings, at least in part due to shortcomings in infection prevention and control (12, 13) .\n",
      "Post-exposure prophylaxis with ribavirin and lopinavir/ritonavir decreased the MERS-CoV risk in health care workers by 40% (14) .\n",
      ">>>{THe eMeRGence oF covId-19 cAused by sARs-cov-2}<<<\n",
      "In mid-December of 2019, a pneumonia outbreak erupted once again in China, in the city of Wuhan, the province of Hube (1).\n",
      "The outbreak spread during the next two months throughout the country, with currently over 80 000 cases and more than 2400 fatal outcomes (CFR 2.5%), according to official reports.\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.821874] in time [70.831763] seconds\n",
      "Title\n",
      "{ COVID-19 coronavirus vaccine design using reverse vaccinology and machine learning}\n",
      "Main body\n",
      "The protein was also predicted to contain promiscuous MHC-I and MHC-II T-cell epitopes, and linear B-cell epitopes localized in specific locations and functional domains of the protein.\n",
      "Our predicted vaccine targets provide new strategies for effective and safe COVID-19 vaccine development.\n",
      ">>>{SARS-CoV-2.}<<<\n",
      "Four of them (HCoV-HKU1, HCoV-OC43, HCoV-229E, and HCoV-NL63) have 46 been circulating in the human population worldwide and cause mild symptoms 2 .\n",
      "Coronavirus 47 and M protein can be delivered as recombinant DNA vaccine and viral vector vaccine (Table 2) conservation suggested the potential of N protein as a candidate for the cross-protective vaccine 119 against SARS and MERS.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is caused by SARS-COV2\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mg58GuxNZXr"
   },
   "source": [
    "### How is COVID-19 spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iro3OuStNk1r",
    "outputId": "870609c5-38f8-445c-9814-e95fb977e46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.793027] in time [70.825008] seconds\n",
      "Title\n",
      "{ Spread of COVID-19 in India: A Simple Algebraic Study}\n",
      "Main body\n",
      ">>>{ Spread of COVID-19 in India: A Simple Algebraic Study}<<<\n",
      " The number of patients, infected with COVID-19, began to increase very rapidly in India from March 2020.\n",
      "The country was put under lockdown from 25 March 2020.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.715870] in time [28.922324] seconds\n",
      "Title\n",
      "{ COVID-19: Spatial Analysis of Hospital Case-Fatality Rate in France}\n",
      "Main body\n",
      "Lethality depends on the intrinsic virulence of the virus but, unlike morbidity, it does 56 not depend on its contagiousness.\n",
      "Virulence comes from the reproductive capacity of 57 the virus in the cell, its capacity for cellular degradation, and its ability to induce or not 58 an innate or specific immune response.\n",
      ">>>{Virulence is of purely biological origin and once 59 the virus has entered the target cell where it will cause its pathogenic effect does no 60 longer depends on environmental conditions outside the host.}<<<\n",
      "Virulence is independent 61 of the host population, but may change over time and space if there is a risk of natural 62 mutation/selection of the pathogen.\n",
      "Contagiousness characterizes the biological 63 capacity of the virus to reach the target cell system of its host, and the ability to be 64 transmitted from one individual to another.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.715217] in time [94.848793] seconds\n",
      "Title\n",
      "{ All about COVID-19 in brief}\n",
      "Main body\n",
      ">>>{ All about COVID-19 in brief}<<<\n",
      " NaN\n",
      "A new coronavirus was discovered due to detection of an unfamiliar pneumonia in a group of patients in December 2019 in Wuhan, China, initially named as 2019 novel coronavirus (2019-nCoV) by the World Health Organization (WHO) on 7 January.\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.849790] in time [70.838116] seconds\n",
      "Title\n",
      "{ Two mechanisms for accelerated diffusion of COVID-19 outbreaks in regions with high intensity of population and polluting industrialization: the air pollution-to-human and human-to-human transmission dynamics}\n",
      "Main body\n",
      ">>>{ What is COVID-19?}<<<\n",
      "Coronavirus disease 2019 (COVID-19) is viral infection that generates a severe acute respiratory syndrome with serious pneumonia that may result in progressive respiratory failure and death.\n",
      "What are the goals of this investigation?\n"
     ]
    }
   ],
   "source": [
    "question = \"How is COVID-19 spread\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrj8LHhKNcuE"
   },
   "source": [
    "### Where was COVID-19 discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79X3Amf-NlWK",
    "outputId": "2f32edff-74aa-456b-fd8e-95ebbb8542b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.782045] in time [70.834580] seconds\n",
      "Title\n",
      "{ Exploring the spread dynamics of COVID-19 inMorocco}\n",
      "Main body\n",
      ">>>{ Exploring the spread dynamics of COVID-19 inMorocco}<<<\n",
      " Despite some similarities of the dynamic of COVID-19 spread in Morocco and other countries, the infection, recovery and death rates remain very variable.\n",
      "In this paper, we analyze the spread dynamics of COVID-19 in Morocco within a standard susceptible-exposed-infected-recovered-death (SEIRD) model.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.600120] in time [28.832526] seconds\n",
      "Title\n",
      "{ Arguing from Ignorance}\n",
      "Main body\n",
      "This is the hallmark of all heuristics.\n",
      "They are 'fast and frugal' procedures that do not expend the resources of their more systematic counterparts in reasoning (Gigerenzer and Goldstein 1996) .\n",
      ">>>{The scientist or health worker who must respond to an emerging infectious disease or other health problem can employ heuristics as a rational strategy for managing decision-making under the pressure of the practical sphere.}<<<\n",
      "The termination of deliberation and subsequent acceptance of a claim that the ignorance heuristic makes possible gives investigators some epistemic foothold in an incomplete evidential base.\n",
      "That foothold permits decisions and actions to be taken in the absence of evidence, such as when the human SBO ban was instituted in advance of any knowledge of the transmissibility of BSE to humans.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.636963] in time [94.910723] seconds\n",
      "Title\n",
      "{ Reimagining the Administrative State in Times of Global Health Crisis: An Anatomy of Taiwan’s Regulatory Actions in Response to the COVID-19 Pandemic}\n",
      "Main body\n",
      "Last but not least, there is no withdrawal mechanism either, which would set out the conditions for when the government should roll back the invasive programmes created in the name of public health.\n",
      "This mechanism is important because the data surveillance measures might long outlive the COVID-19 crisis.\n",
      ">>>{COVID-19.}<<<\n",
      "75 The bottom line is fairly straightforward: while some exceptional measures may be justified by a \"legality of emergency\", 76 ignoring existing rule of law and human rights safeguards seems to be a dangerous move and may ultimately institutionalise exceptionalism and leave a lasting impact on the legal system and culture such as public health and in a time of emergency.\n",
      "80 Yet, even so, can we harness the administrative state with only traditional safeguards such as due process, proportionality and judicial review?\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.885913] in time [70.835836] seconds\n",
      "Title\n",
      "{ Mental health care for international Chinese students affected by the COVID-19 outbreak}\n",
      "Main body\n",
      " NaN\n",
      ">>>{, responsible for COVID-19.}<<<\n",
      "They also face discrimination and isolation in some countries due to being deemed as potential SARS-CoV-2 carriers.\n",
      "1 Some media outlets have used derogatory headlines, perpetuating stereotypes and prejudices about Chinese people.\n"
     ]
    }
   ],
   "source": [
    "question = \"Where was COVID-19 discovered\"\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL7s_hXJNhhm"
   },
   "source": [
    "### How does coronavirus spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFtqt6vwNlv4",
    "outputId": "09a8d638-4b83-400a-dabf-8d84df84c484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************BERT METHOD********************\n",
      "Document found with similarity [0.763760] in time [70.987698] seconds\n",
      "Title\n",
      "{ Coronaviruses pandemics: Can neutralizing antibodies help?}\n",
      "Main body\n",
      ">>>{ Coronaviruses pandemics: Can neutralizing antibodies help?}<<<\n",
      " Abstract For the first time in Homo sapiens history, possibly, most of human activities is stopped by coronavirus disease 2019 (COVID-19).\n",
      "Nearly eight billion people of this world are facing a great challenge, maybe not “to be or not to be” yet, but unpredictable.\n",
      "\n",
      "\n",
      "********************GloVe METHOD********************\n",
      "Document found with similarity [0.703812] in time [28.815904] seconds\n",
      "Title\n",
      "{ In silico approach toward the identification of unique peptides from viral protein infection: Application to COVID-19}\n",
      "Main body\n",
      "As shown in Figure 2A more peptides were identified from the nucleocapsid protein than any other protein, followed by the Spike protein.\n",
      "When the first SARS-CoV-2 study was released we reprocessed this study and observed the same trend, as shown in Figure 2B .\n",
      ">>>{Although not strictly quantitative, counting the number of peptides identified in a global proteomics experiment has been historically used as a metric for approximating relative protein abundance in a sample.}<<<\n",
      "These results suggest that the Nucleoprotein and Spike Glycoprotein may be the highest abundance SARS-CoV-2 proteins present in human infections\n",
      "To identify peptides likely to make poor targets due to genomic variability, we performed a 2-stage approach.\n",
      "\n",
      "\n",
      "********************COVID TWITER BERT METHOD********************\n",
      "Document found with similarity [0.571821] in time [94.922793] seconds\n",
      "Title\n",
      "{ Nature of transmission of Covid19 in India}\n",
      "Main body\n",
      ">>>{ Nature of transmission of Covid19 in India}<<<\n",
      " We examine available data on the number of individuals infected by the Covid-19 virus, across several different states in India, over the period January 30, 2020 to April 10, 2020.\n",
      "It is found that the growth of the number of infected individuals $N(t)$ can be modeled across different states with a simple linear function $N(t)=\\gamma+\\alpha t$ beyond the date when reasonable number of individuals were tested (and when a countrywide lockdown was imposed).\n",
      "\n",
      "\n",
      "********************SBERT METHOD********************\n",
      "Document found with similarity [0.765933] in time [70.813619] seconds\n",
      "Title\n",
      "{ Coronavirus, as a source of pandemic pathogens}\n",
      "Main body\n",
      ">>>{ Coronavirus, as a source of pandemic pathogens}<<<\n",
      " The coronavirus and the influenza virus have similarities and differences.\n",
      "In order to comprehensively compare them, their genome sequencing data were examined by principal component analysis.\n"
     ]
    }
   ],
   "source": [
    "question = \"How does coronavirus spread\"\n",
    "\n",
    "\n",
    "print(\"********************BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, BERT_model, dataset, BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************GloVe METHOD********************\")\n",
    "get_similar_title_and_document(question, GLOVE_model, dataset, GLOVE_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************COVID TWITER BERT METHOD********************\")\n",
    "get_similar_title_and_document(question, COVID_TWITER_BERT_model, dataset, COVID_TWITER_BERT_embeddings_dataset)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"********************SBERT METHOD********************\")\n",
    "get_similar_title_and_document(question, SBERT_model, dataset, SBERT_embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHbXeZ4JK4NC"
   },
   "source": [
    "# Evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLMN2Q9XOG6E"
   },
   "source": [
    "As for the evaluation, we will evaluate the models based on the time we needed to compute the embeddings and to find the best sentence and also we will evaluate them based on the relativity of the answers that we took from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca9ETDXCT7FY"
   },
   "source": [
    "## Computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rej1G2NvT4yL"
   },
   "source": [
    "For the computation time, it is very obvious that the bigger the model the more time it takes to calculate the embeddings. In more detail, the BERT together with the SBERT models took about the same time as they are models of approximately the same size with 768 dimensions. GLOVE needed the least time to compute as it is a simple neural network with 1 layer and its representations had only 300 dimensions. Finally, the pre-trained model on Twitter in texts related to COVID-19 was very difficult to run. To calculate the representations needed more than 6 hours. \n",
    "\n",
    "About the time the models took to find a best-matched article, the search time is proportional to the size of the embeddings. BERT and SBERT having the same time having 768 dimensions were in the middle, GLOVE was the fastest as it has only 300 dimensions, and Twitter pre-trained was the slowest as it has 1024 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49qVWKQLULSe"
   },
   "source": [
    "## Relativity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ6UJkxoUj66"
   },
   "source": [
    "The relevance between the questions and the results was not quite satisfactory.\n",
    "\n",
    " More specifically, GLOVE had the worst results. It may have been the fastest but the statistical relationship between the words is not enough to describe their meaning. \n",
    "\n",
    "After GLOVE in stagnation comes the simple BERT. The output of the final layer proved to be good enough to semantically describe the content of all the sentences. In some cases, in fact, it has better proposals than all the other BERT models that were modified better for the this task.\n",
    "\n",
    "\n",
    "\n",
    "Finally, the last two models, the S-BERT and the Twitter pre-trained BERT give the best results. Their differences are small. The Twitter pre-trained BERT, as it was trained in relevant texts, includes in its vocabulary the unusual words for the coronavirus, and several times it seems to work like a bag of words. With a slight difference, the S-BERT is better as its sentences have a relation between them and the question. S-BERT had been trained for this very reason so it would be logical to have the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8lsCIXJ1EYc"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN1fceah1JDE"
   },
   "source": [
    "\n",
    "One reason we have could be that the models have not been trained in scientific medical records that relate exclusively to COVID19. Another reason perhaps more important could be the way we choose the most relevant document. Cosine similarity is a very simple and greedy way the like does not receive any semantic information among the vectors. Maybe some of the proposals had a greater relationship with each other and just did not have the better cosine similarity. This evaluation method leaves the models to do the heavy work for it, but the models proved unable for this task.\n",
    "\n",
    "One way to improve search performance could be to further train these models on COVID19 data. Also, another way would be to use a different metric of cosine similarity to compare the different embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvTgjhBCWqVd"
   },
   "source": [
    "# Resources\n",
    "\n",
    "CORD-19\n",
    "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.1.pdf\n",
    "\n",
    "S-BERT\n",
    "https://arxiv.org/pdf/1908.100Bert84.pdf\n",
    "\n",
    "GloVe\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Ηuggingface library\n",
    "https://huggingface.co/bert-base-uncased\n",
    "\n",
    "ΒΕΡΤ model\n",
    "https://arxiv.org/abs/1810.04805\n",
    "\n",
    "BERT pretrained on Twiiter for COVID related tweets\n",
    "https://github.com/digitalepidemiologylab/covid-twitter-bert\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TkJxt4yTPOBA",
    "YRSC5XS2fqdI",
    "ZzZ3XQuBqKBi",
    "0HMJaU4FPYNP",
    "7ct2PLLePSlU",
    "1hWgY7n0PZ01",
    "J4FW94wXPubW",
    "v45TeaDqPgVz",
    "MUlkqUKaVud_",
    "yF3sBuw9ChIX",
    "Bek-jgGPChId",
    "QvTgjhBCWqVd"
   ],
   "name": "project4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
